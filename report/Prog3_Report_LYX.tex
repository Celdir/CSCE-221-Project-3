%% LyX 2.1.4 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass{article}
\usepackage[latin9]{inputenc}
\usepackage{amsmath}
\begin{document}

\title{\textbf{Programming 2 Assignment}}


\author{Michael Earl, Gerald Hu \\
 \\
 CSCE 221-200}


\date{April 11, 2016}

\maketitle

\section*{Introduction}

\noindent The purpose of this assignment was to analyze the theoretical
and actual performance of several common sorting algorithms discussed
in class. The project implemented bubble sort, two ``slow sorting''
algorithms (insertion and selection sort), and two ``fast sorting''
algorithms (mergesort and quicksort). The running time of these algorithms
was analyzed for several types of data: sorted data, reverse-sorted
data, randomized data, and data with many similar elements. {[}TODO
- summary of results + what we learned{]}\\



\section*{Implementation Details}

\noindent All sorting algorithms took random-access iterators to the
first and last elements of the container, and a comparator. \\


\noindent Bubble Sort is a simple sorting algorithm. It compares a
set of two elements that are next to each other, and if the elements
in the bubble are out of order, they are swapped. This set traverses
the entire container from start to end, then starts from the beginning
again, repeating until there are no more swaps to make.\\


\noindent Selection Sort is one of the two ``slow'' sorting algorithms.
It traverses the entire container searching for the smallest element,
then swaps that element into the first position. Then it searches
the remainder of the container for the next-smallest element, then
swaps that element into the first position of the remainder. On its
$i$th traversal, it will swap the smallest element into the $i$th
position. This process continues $n$ times, at which point there
is no more ''remainder'' of the container to traverse.\\


\noindent Insertion Sort is the other of the two ``slow'' sorting
algorithms. It traverses the entire container $n$ times. In each
traversal, for each step $i$, if $i$ is not the first element, and
if $i$ and $i-1$ are out of order, the two are swapped, swapping
the element into an ''already-sorted'' portion of the container.
Each traversal will grow the already-sorted portion, until all the
data is in the sorted portion. \\


\noindent Mergesort is one of the two ``fast'' sorting algorithms.
It splits the input data into two subarrays, and recursively splits
each of those subarrays into two subarrays, repeating the splitting
process until each subarray is of size$<$2. These subarrays are then
sorted in sets of two, and the resulting sorted data is put into an
array and returned. The returned sorted data is merged with more data
using the same merge function, until fully-sorted data is returned.\\


\noindent Quicksort is the other of the two ``fast'' sorting algorithms.
It randomly selects an element of the data to use as a ``pivot''
, and traverses the entire container. It moves all elements less than
the pivot to before the pivot's position, and moves all elements greater
than the pivot to after the pivot's position, all of which is done
in-place. This places the pivot in the proper position. Then, quicksort
is recursively called, once on the set of elements less than the pivot,
and once on the set of elements greater than the pivot. \\


\noindent The skeleton of timing.cpp was provided by the instructors.
The timing function was already implemented for randomly sorted input
data; we modified it to analyze sorted and reverse-sorted data, as
well as data with many similar elements. The timing function relies
on high\_resolution\_clock; given a sorting function and a number
$n$, the timing function would generate a vector with $n$ elements
in it. Then, the timing function would try to sort the vector using
the specified sorting algorithm. This process was repeated for increasing
sizes of $n$, and repeated 10 times at each $n$ to ensure an accurate
average time. {[}TODO - what we learned{]}\\



\section*{Theoretical Analysis}

\noindent Bubble Sort is $O(n^{2})$ average and worst case. It will
force the largest element to the end after $n$ comparisons; then
it will repeat this for the $n-1$ smallest element, then the $n-2$
smallest element, and so on, repeating $n$ times in total. $n$ traversals
and $n$ comparisons/swaps at each traversal results in $O(n^{2})$
time. Bubble Sort's best case is $O(n)$, in the case that it's already
sorted. It traverses the list once and makes $n$ comparisons, but
does not do any swaps or any further traversals.\\


\noindent Selection Sort is $O(n^{2})$ in all cases. In searching
for the smallest element, it will traverse $n$ elements and make
$n$ comparisons, before forcing the smallest element to the start.
It then repeats this process for a sublist of size $n-1$, then $n-2$...$n-n$.
$\sum_{i=0}^{n}i=(i)(i+1)/2$, which is $O(n^{2})$ behavior. Furthermore,
it will always search for the smallest element, regardless of whether
it needs to be swapped or not, and will perform all traversals regardless
of what data type it's given.\\


\noindent Insertion Sort is $O(n^{2})$ average and worst case, for
similar reasons as Selection Sort: it will traverse a list of size
$n$, then size $n-1$, then $n-2$...$n-n$. Unlike Selection Sort,
Insertion Sort is $O(n)$, in the best case that it is already sorted,
as it will only traverse the list once. \\


\noindent Mergesort is $O(nlog(n))$ in all cases. At each step, the
problem is split in half, until it reaches the smallest possible subproblem;
it will take $O(logn)$ splits to reach the base level, and $O(n)$
to merge all the data back together. There is not a ``best case''
input type, as mergesort will perform the same operations regardless
of the data it's given. \\


\noindent Quicksort is $O(nlog(n))$ average case and best case, and
$O(n^{2})$ in a rare worst case. The best case of quicksort is $O(nlogn)$
due to an inherent limit on comparison-based sorting algorithms. Assuming
an ``average case'' where the pivots selected give equal (or close
to equal) sized sublists, it will take $O(logn)$ splits to reach
the base level, and $O(n)$ to merge all the data back together. The
worst case is if every pivot chosen is a bad selection, splitting
into a subproblem of size 1 and size $(n-1)$, which is then split
into a subproblem of size 1 and size $(n-2)$, so on....resulting
in $O(n)$ splits. \\



\section*{Experimental Setup}

Timing tests were conducted using the provided timing.cpp, compiled
with the provided makefile's commands. Compilation was done on Michael's
desktop, with G++ version 5.3.0. Compilation was set to the C++14
standard, with the -G flag enabled and O2 optimization level, warnings
set to -Wall -Werror (warn all and all warnings treated as compilation
errors), and dependencies flagged with -MMD (auto-generate dependencies).\\


\noindent Tests were run on Michael's desktop, which runs Arch Linux
x86\_64 version 4.4.5-1. It has 8 total gigabytes of RAM. It uses
an AMD FX-8350 8-Core processor, with a clock speed of 4 GHz. \\


\noindent Timing functions output timing results for input sizes that
were powers of 2, starting from 2 itself, and ending at a maximum
size specified by the user. Each step of the timing was repeated 10
times, and the average of each result taken. Bubble Sort went up to
a maximum input size of 32768 $(2^{15})$ elements; Insertion and
Selection sorts went up to a maximum input size of 131072 $(2^{17})$
elements; and Quicksort and Mergesort went up to a maximum input size
of 4194304 $(2^{22})$ elements.\\


\noindent Four data types were used in testing: sorted data, reverse-sorted
data, randomly-sorted data, and data with ``few unique elements''.
For our purposes, ``data with few unique elements'' was assumed
to mean ``data where a lot of the elements are the same''; the input
data generated consisted mostly of 1s, with a few 2s spaced out in
the data.\\



\section*{Results and Discussion}

{[}TODO - this{]}


\section*{Conclusion}

This assignment achieved a few main goals. First and foremost, it
deepened our understanding of the way maps and binary search trees
work. It also introduced, in a practical way, how to use AVL trees.
And it verified the theoretical discussions held in class about the
performance of functions of the ADT implementation. The advantages
of the AVL tree were concretely seen in the performance difference
between the regular binary search tree and the AVL tree. From the
timing results obtained in this assignment, it can be seen that AVL
trees are better than or on par with binary search trees in every
situation. Overall, this assignment as a whole gave needed practice
in coding as well as reinforced concepts that were learned in class. 

This assignment showed the differences in runtime between various
sorting algorithms, highlighting which ones were theoretically more
efficient, and which ones actually performed better in practice. Quicksort,
like the discussions in class, consistently performed well and sorted
quickly; Insertion Sort and Bubble Sort ran quickly on already-sorted
data, but performed worse in other areas. Out of the slow sorts, Insertion
was the best, and Bubble was the worst in most cases. Out of all sorting
algorithms, Quicksort and Mergesort performed similarly, but Quicksort
was a bit faster. 
\end{document}
